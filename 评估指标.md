# Result-Evaluation
* True Positive(真正, TP)：将正类预测为正类数.
* True Negative(真负 , TN)：将负类预测为负类数.
* False Positive(假正, FP)：将负类预测为正类数.
* False Negative(假负 , FN)：将正类预测为负类数.
* 精确率 P = TP / (TP+FP)
* 准确率(accuracy) ACC = (TP + TN) / (TP + TN + FP + FN)  
* 召回率 **Recall** = TP / (TP+FN) 
* 精确率和召回率的调和均值 2 / F1 = 1 / P + 1 / R 
* F1 = 2TP / (2TP + FP + FN)
* 平均错误率(balanced error rate): BER = 1 / 2 (FP / (FP + TN) + FN / (FN + TP))
* Mattherws相关系数(Matthews correlation coefficient): 
* MCC = (TP * TN - FP * FN) /  √￣(TP + FP)(TP + FN)(TN + FP)(TN + FN)
* **Sensitivity** or true positive rate(TPR): TPR = TP / (TP + FN)
* **Specificity** (SPC) or true negative rate: SPC = TN / N = TN / (TN + FP) 
* precision or positive predictive value (**PPV**): PPV = TP / (TP + FP)
* negative predictive value (**NPV**): NPV = TN / (TN + FN) 
* False Positive Rate (**FPR**): FPR = FP / (FP + TN) = 1 - SPC
* false negative rate (**FNR**): FNR = FN / (TP + FN) = 1 - TPR
* false discovery rate (**FDR**): FDR = 1- PPV
* accuracy (**ACC**): ACC = (TP + TN) / (TP + TN + FP + FN)  

#### In signal detection theory, a receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. 比如在逻辑回归里面，我们会设一个阈值，大于这个值的为正类，小于这个值为负类。如果我们减小这个阀值，那么更多的样本会被识别为正类。这会提高正类的识别率，但同时也会使得更多的负类被错误识别为正类。为了形象化这一变化，在此引入 ROC ，ROC 曲线可以用于评价一个分类器好坏。
### ROC 曲线的两个指标: 
1. True Positive Rate(TPR), TPR = TP / (TP + FN) 
2. False Positive Rate(FPR), FPR = FP / (FP + TN)
直观上，TPR 代表能将正例分对的概率，FPR 代表将负例错分为正例的概率。在 ROC 空间中，每个点的横坐标是 FPR，纵坐标是 TPR，这也就描绘了分类器在 TP（真正率）和 FP（假正率）间的 trade-off2。

* AUC（Area Under Curve）被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。
The AUC value is equivalent to the probability that a randomly chosen positive example is ranked higher than a randomly chosen negative example. 翻译过来就是，随机挑选一个正样本以及一个负样本，分类器判定正样本的值高于负样本的概率就是 AUC 值。简单说：AUC值越大的分类器，正确率越高3。
* AUC=1  
AUC=1，**完美分类器**，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。
* 0.5<AUC<1  
0.5<AUC<1，**优于随机猜测**。这个分类器（模型）妥善设定阈值的话，能有预测价值。
* AUC=0.5  
AUC=0.5，跟**随机猜测**一样（例：丢铜板），模型没有预测价值。
* AUC<0.5  
AUC<0.5，比**随机猜测**还差；但只要总是反预测而行，就优于随机猜测，因此不存在AUC<0.5的情况。
既然已经这么多评价标准，为什么还要使用ROC和AUC呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反）

### Precision-recall 曲线  
如果你想评估一个分类器的性能，一个比较好的方法就是：观察当阈值变化时，Precision与Recall值的变化情况。如果一个分类器的性能比较好，那么它应该有如下的表现：被识别出的图片中TP所占的比重比较大，并且在识别出FP之前，尽可能多地正确识别出TP，也就是让Recall值增长的同时保持Precision的值在一个很高的水平。而性能比较差的分类器可能会损失很多Precision值才能换来Recall值的提高。通常情况下，文章中都会使用Precision-recall曲线，来显示出分类器在Precision与Recall之间的权衡。

